# WebAR Experiment for OLabs  

## Demo Video  
[Watch the Video](https://drive.google.com/file/d/1Wfm-NOINuB0eAEoGixyq7OgRUFwc6Nzk/view?usp=drive_link)  
MindAR Voice-Controlled AR Model  

# WebAR Experiment for OLabs  

## Project Background  
This project was initiated as part of the **OLabs Hackathon**, where our team sett out to enhance **virtual science experiments using WebXR and AR technologies**. Our goal was to make online labs **more interactive and immersive**, helping students better understand scientific concepts.  

### Motivation  
Traditional online lab simulations often lack **hands-on interaction**, making it difficult for students to visualize real-world applications. Our team saw an opportunity to bridge this gap using **Augmented Reality (AR)**. By integrating WebXR, we aimed to create **an accessible, modular, and reusable WebAR framework** for OLabs experiments.  

### Challenges Faced   
ğŸ“Œ **Surface detection & object placement** â€“ Implementing accurate AR object placement took trial and error, especially for realistic refraction simulations.  
ğŸ–¼ï¸ **Image tracking issues** â€“ We faced difficulties with image recognition but plan to refine it further for better experiment interaction.  
âš¡ **Performance optimization** â€“ Ensuring smooth rendering while keeping AR interactions fluid required balancing resource usage and loading times.  
ğŸ™ï¸ **Voice recognition & QR scanning** â€“ Integrating voice control and dynamic QR-based experiment loading needed additional APIs and fine-tuning.  

Despite these challenges, we successfully built a **WebAR-based website** that allows experiments to be dynamically loaded, interactively guided, and voice-controlledâ€”all within a **browser** without requiring external apps.  

## Features  
  
ğŸ§‘â€ğŸ”¬ **Interactive AR objects** â€“ Users can manipulate 3D elements like prisms, lenses, and lab tools.  
ğŸ“‚ **Modular design** â€“ Easily add new experiments using a JSON-based configuration.  
ğŸ¯ **Image tracking** â€“ Detects lab apparatus and positions 3D models accordingly.  
ğŸ“¡ **Surface detection** â€“ Places AR objects on real-world surfaces with accurate alignment.  
ğŸ“¸ **QR code scanning** â€“ Load experiments dynamically by scanning QR codes.  
ğŸ—£ï¸ **Voice recognition** â€“ Enables hands-free experiment interaction using voice commands.  
ğŸ”„ **Dynamic loading** â€“ Fetches and loads experiment data from external JSON files.  
âš¡ **Performance optimized** â€“ Efficient rendering and resource management for smooth AR experiences.  
ğŸŒ **Cross-platform compatibility** â€“ Works on mobile, tablets, and AR/VR headsets with WebXR support.  

## Hosting & Local Development  

### Running a Local Server with Python  
To test WebXR applications locally, we used Pythonâ€™s built-in HTTP server to host the files.  
Run the following command in your project directory:  

**python -m http.server 8000**

This starts a simple HTTP server on localhost:8000. You can then access your project in a browser at:

**http://localhost:8000**

## Using ngrok for Public Access
Since localhost is only accessible on your device, we used ngrok to create a public URL for testing WebXR on mobile devices.

Get the public URL from the ngrok output and open it on your mobile device or share it with teammates for remote testing.




## Scalability & Future Expansion  

ğŸ“ˆ **Cloud-Hosted Experiment Library**  
- Expand the number of experiments dynamically via a cloud-based database.  
- Educators can create and upload their own AR experiments.  

ğŸŒ **Multi-Language Support**  
- Implementing voice commands and instructions in **multiple languages**.  
- Expands accessibility for students across different regions.  

ğŸ–¥ï¸ **PC & AR Headset Support**  
- Expanding compatibility with **HoloLens, Oculus Quest, and AR glasses**.  
- Provides a full immersive experience for high-end devices.  

---


## Technologies Used  
- **WebXR** â€“ Enables immersive AR/VR capabilities in the browser.  
- **Three.js** â€“ Handles 3D rendering and object interactions.  
- **JavaScript & HTML** â€“ Core technologies for frontend development.  
- **JSON-based configuration** â€“ Flexible experiment setup without modifying code.  
- **Web Speech API** â€“ Implements voice recognition for hands-free control.  
- **Zxing.js** â€“ Used for QR code scanning functionality.  
- **Blender** â€“ Used for creating and optimizing 3D models for AR.  
- **GLTF Formats** â€“ Standard format for AR model storage and rendering.  

---


## Screenshots ğŸ“¸  

Here are some **screenshots** showcasing the WebAR framework in action:  


## Augumented Reality

 ![Surface Detection](/images/AR%20(1).png) 
 ![QR Code Scan](/images/QRCode.png) 

 ## Prism Model

 ![Prism](/images/prism1.png) 
 ![Prism](/images/prism2.png) 
 ![Prism](/images/prism3.png) 


## Setup Instructions  

1. **Clone the repository**  
   ```sh
   git clone https://github.com/spsancodes/Mind_AR_Experiments
   cd Mind_AR_Experiments

## Acknowledgments ğŸ™Œ
Would like thank the following individuals and resources for their valuable contributions:  

- ğŸ‘¨â€ğŸ’» **[Teammate 1](https://github.com/Garudan014)**
- ğŸ‘¨â€ğŸ’» **[Teammate 2](https://github.com/Yadi-codes)**
- ğŸ‘¨â€ğŸ’» **[Teammate 3](https://github.com/)**

### Miscelleneous
- |**Mentors in the Hackathon**|
- |**University Advisors and professors**|
- |**[Mr. Pratyaksh](https://github.com/LoneWolf4713)**|


